{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc170894-1e0c-46b5-9b32-dedbb8cc28b9",
   "metadata": {},
   "source": [
    "# **Group 21 - Report**\n",
    "# *Introduction to Artificial Intelligence IN3062*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222a8870",
   "metadata": {},
   "source": [
    "### **Introduction**\n",
    "\n",
    "We decided to opt for a unique dataset that was not found to be studied online, while also providing a problem domain that could be approached with both binary classification and regression models. As such, we decided on a dataset for `Occupancy Detection` that contained continuous real data for regression and a binary problem domain for classification, its variables being:\n",
    "\n",
    "| Variable Name | Units | Data type |\n",
    "| --- | --- | --- |\n",
    "| ID | N/A | Integer |\n",
    "| Temperature | Celcius | Real |\n",
    "| Humidity | Relative % | Real |\n",
    "| Light | Lux | Real |\n",
    "| CO2 | ppm | Real |\n",
    "| Humidity Ratio | Kilogram (water vapour) / Kilogram (air) | Real |\n",
    "| Occupancy | N/A | Binary |\n",
    "\n",
    "With this dataset we are studying the binary variable `Occupancy` with `0` the room is not occupied and `1` meaning the room is occupied. This data was collected using time stamped pictures of the ground at one minute intervals of each entry. The aim is to find which variable / combination of variables are most significant when it comes to determining whether a room will be occupied or not.\n",
    "<br>\n",
    "\n",
    "While it is very difficult to source a dataset that has not been studied at least a couple of times before, the dataset we have chosen only appears to have been cited once according to the 'UC Irvine Machine Learning Repository' (see markdown file) with the research paper itself being cited 490 times (see markdown file). Given that the original research was done using the R Programming Language, it seems suitable to be able to use this dataset to study using Python instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fdb2c0",
   "metadata": {},
   "source": [
    "### **Producing the Data Validation Text File**\n",
    "\n",
    "The original datasets provided contains three files: `datatest.txt`, `datatest2.txt` and `datatraining.txt`. However, we need three different datasets for different purposes; training, testing and validation. Therefore, we split the largest dataset into two parts, one part being the desired validation dataset.\n",
    "\n",
    "| File name | Number of entries | File size |\n",
    "| --- | --- | --- |\n",
    "| `datatest.txt` | 2665 | 198KB |\n",
    "| `datatest2.txt` | 9752 | 692KB |\n",
    "| `datatraining.txt` | 8143 | 590KB |\n",
    "\n",
    "As `datatest2.txt` is both the largest file and is significantly larger than `datatest.txt`, we will split it into the two desired parts, having the new `datatest2.txt` have the same number of entries as `datatest.txt` and the remainder will be used for a new `datavalidation.txt` file. To guarantee the randomness of the entries for each new file for whether the variable `Occupancy` is a `0` or `1`, we will be shuffling the `datatest2.txt` file before splitting it. After this we will be testing the balance of the datasets for the problem domain. Below is `shufflesplit.py` that will shuffle then split the data appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aa08048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New datatest2 file created\n",
      "New datavalidation file created\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Test if in the correct working directory\n",
    "\n",
    "cwd = Path().resolve()\n",
    "\n",
    "if not (cwd / \"src\").is_dir():\n",
    "    os.chdir(cwd.parent)\n",
    "\n",
    "# Shuffle and split\n",
    "\n",
    "def shuffle_and_split(filepath, split_count, datatest2_output, datavalidation_output):\n",
    "    \n",
    "    # Open the file and read its contents\n",
    "    \n",
    "    with open(filepath, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "    # Set the header line and the lines to be shuffled\n",
    "    \n",
    "    vars_header = lines[0]\n",
    "    data_lines = lines[1:]\n",
    "    \n",
    "    # Shuffle the lines\n",
    "    \n",
    "    random.shuffle(data_lines)\n",
    "    \n",
    "    # Split the lines and make two different halves of the split index\n",
    "    \n",
    "    lines_datatest2 = [vars_header] + data_lines[:split_count]\n",
    "    lines_datavalidation = [vars_header] + data_lines[split_count:]\n",
    "    \n",
    "    # Write the each set of lines to their output files\n",
    "    \n",
    "    with open(datatest2_output, 'w') as datatest2:\n",
    "        datatest2.writelines(lines_datatest2)\n",
    "        print(\"New datatest2 file created\")\n",
    "    \n",
    "    with open(datavalidation_output, 'w') as datavalidation:\n",
    "        datavalidation.writelines(lines_datavalidation)\n",
    "        print(\"New datavalidation file created\")\n",
    "       \n",
    "# Split count = number of lines for datatest.txt\n",
    "\n",
    "split_count = 2665\n",
    "        \n",
    "# Files\n",
    "\n",
    "input_file = 'datasets\\datatest2.txt'\n",
    "file_datatest2_new = 'datasets\\datatest2_new.txt'\n",
    "file_datavalidation = 'datasets\\datavalidation.txt'\n",
    "\n",
    "shuffle_and_split(input_file, split_count, file_datatest2_new, file_datavalidation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c29b9c",
   "metadata": {},
   "source": [
    "We will now validate the distribution of the `Occupancy` variable using Pandas to visualise this across all the datasets, aiming for at most an 85:15 imbalance for `Occupancy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9b54bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets\\datatest.txt Occupancy Distribution:\n",
      "Total 0s: 1693\n",
      "Total 1s: 972\n",
      "Ratio (True:False) = 36.47:63.53\n",
      "datasets\\datatest2_new.txt Occupancy Distribution:\n",
      "Total 0s: 2125\n",
      "Total 1s: 540\n",
      "Ratio (True:False) = 20.26:79.74\n",
      "datasets\\datatraining.txt Occupancy Distribution:\n",
      "Total 0s: 6414\n",
      "Total 1s: 1729\n",
      "Ratio (True:False) = 21.23:78.77\n",
      "datasets\\datavalidation.txt Occupancy Distribution:\n",
      "Total 0s: 5578\n",
      "Total 1s: 1509\n",
      "Ratio (True:False) = 21.29:78.71\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Test if in the correct working directory\n",
    "\n",
    "cwd = Path().resolve()\n",
    "\n",
    "if not (cwd / \"src\").is_dir():\n",
    "    os.chdir(cwd.parent)\n",
    "\n",
    "# Visualise ratio\n",
    "# Testing for the difference in Occupancy values\n",
    "\n",
    "def visualise_distribution(filepath, test_class = \"Occupancy\"):\n",
    "    \n",
    "    # Read file\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Find the sum of all 0s and 1s for each file\n",
    "    \n",
    "    count_0s = (df[test_class] == 0).sum()\n",
    "    count_1s = (df[test_class] == 1).sum()\n",
    "    \n",
    "    # Print the differences\n",
    "    \n",
    "    print(f\"{filepath} {test_class} Distribution:\")\n",
    "    print(f\"Total 0s: {count_0s}\")\n",
    "    print(f\"Total 1s: {count_1s}\")\n",
    "    \n",
    "    # Calculate as a ratio\n",
    "    \n",
    "    perc_0s = round(((count_0s) / (count_0s + count_1s)) * 100, 2)\n",
    "    perc_1s = round(((count_1s) / (count_0s + count_1s)) * 100, 2)\n",
    "    \n",
    "    print(f\"Ratio (True:False) = {perc_1s}:{perc_0s}\")\n",
    "    \n",
    "\n",
    "# Files\n",
    "\n",
    "datatest_file = 'datasets\\datatest.txt'\n",
    "datatest2_file = 'datasets\\datatest2_new.txt'\n",
    "datatraining_file = 'datasets\\datatraining.txt'\n",
    "datavalidation_file = 'datasets\\datavalidation.txt'\n",
    "\n",
    "visualise_distribution(datatest_file)\n",
    "visualise_distribution(datatest2_file)\n",
    "visualise_distribution(datatraining_file)\n",
    "visualise_distribution(datavalidation_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77781716",
   "metadata": {},
   "source": [
    "As seen in the output, most of the files are close to the previously specified 85:15 boundary. Therefore we will be using the `SMOTE` library (Synthetic Minority Oversampling Technique) which is used to produce similar artificial values for the minority `Occupancy = 1` so that the class distribution becomes balanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fea3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b898bda",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Metrics used for AI Models**\n",
    "\n",
    "describe with relevant hyperparameters:\n",
    "\n",
    "\n",
    "CLASSIFICATION::\n",
    "\n",
    "//\n",
    "\n",
    "accuracy = (num. correct predictions) / total predictions\n",
    "\n",
    "proportion of correct predictions over total predictions\n",
    "\n",
    "works well if the dataset is balanced\n",
    "\n",
    "misleading for imbalanced datasets (e.g., 99% accuracy on a dataset with 99% negatives)\n",
    "\n",
    "//\n",
    "\n",
    "precision = (true positives) / ((true positives) + (false positives))\n",
    "\n",
    "proportion of true positive predictions among all positive predictions\n",
    "\n",
    "important when false positives have high costs (e.g., spam detection)\n",
    "\n",
    "//\n",
    "\n",
    "recall = (true positives) / ((true positives) + (false negatives))\n",
    "\n",
    "sensitivity (true positive rate)\n",
    "\n",
    "proportion of true positives among all actual positives\n",
    "\n",
    "crucial when false negatives are costly (e.g., medical diagnoses)\n",
    "\n",
    "//\n",
    "\n",
    "f1-score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "mean of precision and recall\n",
    "\n",
    "a good model has a balanced precision and recall\n",
    "\n",
    "//\n",
    "\n",
    "reciever operating characteristic = area under the curve\n",
    "\n",
    "measures the trade-off between true positive rate (recall) and false positive rate\n",
    "\n",
    "evaluates model performance across all classification thresholds\n",
    "\n",
    "//\n",
    "\n",
    "REGRESSION::\n",
    "\n",
    "//\n",
    "\n",
    "mean absolute error = average absolute difference between predicted and actual values\n",
    "\n",
    "metric for average error\n",
    "\n",
    "//\n",
    "\n",
    "mean squared error = average squared difference between predicted and actual values\n",
    "\n",
    "penalizes larger errors more heavily than mean absolute error\n",
    "\n",
    "//\n",
    "\n",
    "root mean squared error = square root of mean squared error\n",
    "\n",
    "interpretation in the same units as the target variable\n",
    "\n",
    "//\n",
    "\n",
    "r^2 score = proportion of variance in the target variable explained by the model\n",
    "\n",
    "indicates how well the model fits the data\n",
    "\n",
    "//"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae2ba15",
   "metadata": {},
   "source": [
    "### **Baseline Model Performance**\n",
    "\n",
    "code for very basic perception model and what results it produced\n",
    "\n",
    "map the confusion matrix\n",
    "\n",
    "graph metrics of the baseline and model and explain\n",
    "\n",
    "show hyperparameter tweaking of baseline model and describe changes in metrics from results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5f1aee",
   "metadata": {},
   "source": [
    "### **Chosen Models and Pre-Processing Methods**\n",
    "\n",
    "describe the models to be used (Both classification and regression models as we have a dataset with both non-real and real data)\n",
    "\n",
    "discuss the models strengths and weaknesses for our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15370f46",
   "metadata": {},
   "source": [
    "### **Method 1**\n",
    "\n",
    "code + hyperparameter descriptions\n",
    "\n",
    "tests + metric results + hyperparameter tweaking\n",
    "\n",
    "repeat a couple of times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0fc98d",
   "metadata": {},
   "source": [
    "### **Method 2**\n",
    "\n",
    "code + hyperparameter descriptions\n",
    "\n",
    "tests + metric results + hyperparameter tweaking\n",
    "\n",
    "repeat a couple of times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c2b6c",
   "metadata": {},
   "source": [
    "### **Method 3**\n",
    "\n",
    "code + hyperparameter descriptions\n",
    "\n",
    "tests + metric results + hyperparameter tweaking\n",
    "\n",
    "repeat a couple of times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ccb3ef",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "which method performed the best\n",
    "\n",
    "what was gained in the trade offs for each one and how much did that benefit the model\n",
    "\n",
    "what were the hyperparameters which made a big difference to the different models for each method\n",
    "\n",
    "state which model overall was the most performant in forming a correct output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
